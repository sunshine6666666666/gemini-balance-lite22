/**
 * OpenAIå…¼å®¹é€‚é…å™¨ - Edge Runtimeä¼˜åŒ–ç‰ˆæœ¬
 * ä¸“æ³¨äºOpenAIåˆ°Geminiçš„æ ¼å¼è½¬æ¢ï¼Œç¡®ä¿Edge Runtimeå…¼å®¹æ€§
 */

// å¯¼å…¥å·¥å…·å‡½æ•° - ä½¿ç”¨æ˜ç¡®çš„å¯¼å…¥è¯­æ³•
import * as utils from './utils.js';

/**
 * OpenAIå…¼å®¹APIä¸»å…¥å£
 */
const openai = {
  async fetch(request) {
    const reqId = utils.generateRequestId();
    const startTime = Date.now();
    const url = new URL(request.url);
    
    try {
      utils.logRequestStart(reqId, request.method, url.pathname, 0);

      // è·å–API Key
      const auth = request.headers.get("Authorization");
      const apiKey = auth?.split(" ")[1];
      if (!apiKey) {
        throw new Error('ç¼ºå°‘API Key');
      }

      // è·å–æœ‰æ•ˆçš„API Keyæ± 
      const apiKeys = utils.getEffectiveApiKeys(apiKey, 'OpenAIæ¨¡å¼: ');
      console.log(`ğŸ”‘ [${reqId}] OpenAIæ¨¡å¼è·å¾—${apiKeys.length}ä¸ªæœ‰æ•ˆAPI Key`);

      // è·¯ç”±å¤„ç†
      if (url.pathname.endsWith("/chat/completions")) {
        console.log(`ğŸ’¬ [${reqId}] å¤„ç†èŠå¤©å®Œæˆè¯·æ±‚`);
        const chatBody = await request.json();
        return await handleChatCompletions(chatBody, apiKeys, reqId);
      }

      if (url.pathname.endsWith("/models")) {
        console.log(`ğŸ“‹ [${reqId}] å¤„ç†æ¨¡å‹åˆ—è¡¨è¯·æ±‚`);
        return handleModels();
      }

      throw new Error(`ä¸æ”¯æŒçš„ç«¯ç‚¹: ${url.pathname}`);

    } catch (error) {
      utils.logError(reqId, error, 'OpenAIé€‚é…å™¨');
      const totalTime = Date.now() - startTime;
      utils.logRequestEnd(reqId, 500, totalTime);
      
      return utils.addCorsHeaders(new Response(utils.safeJsonStringify({
        error: {
          message: error.message,
          type: "invalid_request_error"
        }
      }), {
        status: 500,
        headers: { 'Content-Type': 'application/json' }
      }));
    }
  }
};

/**
 * å¤„ç†èŠå¤©å®Œæˆè¯·æ±‚ - OpenAIåˆ°Geminiæ ¼å¼è½¬æ¢
 */
async function handleChatCompletions(openaiRequest, apiKeys, reqId) {
  console.log(`ğŸ”„ [${reqId}] å¼€å§‹OpenAIåˆ°Geminiæ ¼å¼è½¬æ¢`);

  // è½¬æ¢æ¶ˆæ¯æ ¼å¼
  const geminiContents = openaiRequest.messages.map(msg => ({
    role: msg.role === 'assistant' ? 'model' : msg.role,
    parts: [{ text: msg.content }]
  }));

  // æ„å»ºGeminiè¯·æ±‚
  const geminiRequest = {
    contents: geminiContents,
    generationConfig: {
      temperature: openaiRequest.temperature || 0.7,
      maxOutputTokens: openaiRequest.max_tokens || 1024,
      topP: openaiRequest.top_p || 1.0
    }
  };

  console.log(`ğŸ“ [${reqId}] è½¬æ¢å®Œæˆ: ${geminiContents.length}æ¡æ¶ˆæ¯`);

  // ç¡®å®šæ˜¯å¦ä¸ºæµå¼è¯·æ±‚
  const isStreaming = openaiRequest.stream === true;
  const endpoint = isStreaming ? 'streamGenerateContent' : 'generateContent';
  const model = mapOpenAIModelToGemini(openaiRequest.model);
  
  const targetUrl = `https://generativelanguage.googleapis.com/v1beta/models/${model}:${endpoint}`;
  utils.logLLMRequest(reqId, targetUrl, model, isStreaming);

  // å‘é€è¯·æ±‚åˆ°Gemini
  const response = await utils.enhancedFetch(
    targetUrl,
    {
      method: 'POST',
      headers: { 'Content-Type': 'application/json' },
      body: utils.safeJsonStringify(geminiRequest)
    },
    apiKeys,
    reqId,
    'OpenAI->Gemini'
  );

  // å¤„ç†å“åº”
  if (isStreaming) {
    return handleStreamingResponse(response, reqId);
  } else {
    return handleNormalResponse(response, reqId);
  }
}

/**
 * å¤„ç†æ™®é€šå“åº” - Geminiåˆ°OpenAIæ ¼å¼è½¬æ¢
 */
async function handleNormalResponse(response, reqId) {
  const geminiResponse = await response.json();
  console.log(`ğŸ”„ [${reqId}] å¼€å§‹Geminiåˆ°OpenAIæ ¼å¼è½¬æ¢`);

  // è½¬æ¢ä¸ºOpenAIæ ¼å¼
  const openaiResponse = {
    id: `chatcmpl-${reqId}`,
    object: "chat.completion",
    created: Math.floor(Date.now() / 1000),
    model: "gpt-3.5-turbo",
    choices: [{
      index: 0,
      message: {
        role: "assistant",
        content: geminiResponse.candidates?.[0]?.content?.parts?.[0]?.text || "æŠ±æ­‰ï¼Œæ— æ³•ç”Ÿæˆå›å¤ã€‚"
      },
      finish_reason: "stop"
    }],
    usage: {
      prompt_tokens: 0,
      completion_tokens: 0,
      total_tokens: 0
    }
  };

  console.log(`âœ… [${reqId}] æ ¼å¼è½¬æ¢å®Œæˆ`);
  
  return utils.addCorsHeaders(new Response(utils.safeJsonStringify(openaiResponse), {
    status: 200,
    headers: { 'Content-Type': 'application/json' }
  }));
}

/**
 * å¤„ç†æµå¼å“åº” - Gemini SSEåˆ°OpenAI SSEæ ¼å¼è½¬æ¢
 */
async function handleStreamingResponse(response, reqId) {
  console.log(`ğŸŒŠ [${reqId}] å¼€å§‹æµå¼å“åº”è½¬æ¢`);

  const encoder = new TextEncoder();
  const decoder = new TextDecoder();

  const stream = new ReadableStream({
    async start(controller) {
      const reader = response.body.getReader();
      
      try {
        while (true) {
          const { done, value } = await reader.read();
          if (done) break;

          const chunk = decoder.decode(value);
          const lines = chunk.split('\n');

          for (const line of lines) {
            if (line.startsWith('data: ')) {
              const data = line.slice(6);
              if (data === '[DONE]') {
                controller.enqueue(encoder.encode('data: [DONE]\n\n'));
                continue;
              }

              try {
                const geminiData = JSON.parse(data);
                const openaiData = {
                  id: `chatcmpl-${reqId}`,
                  object: "chat.completion.chunk",
                  created: Math.floor(Date.now() / 1000),
                  model: "gpt-3.5-turbo",
                  choices: [{
                    index: 0,
                    delta: {
                      content: geminiData.candidates?.[0]?.content?.parts?.[0]?.text || ""
                    },
                    finish_reason: null
                  }]
                };

                controller.enqueue(encoder.encode(`data: ${utils.safeJsonStringify(openaiData)}\n\n`));
              } catch (e) {
                console.log(`âš ï¸ [${reqId}] æµå¼æ•°æ®è§£æé”™è¯¯: ${e.message}`);
              }
            }
          }
        }
      } catch (error) {
        console.log(`âŒ [${reqId}] æµå¼å¤„ç†é”™è¯¯: ${error.message}`);
      } finally {
        controller.close();
      }
    }
  });

  return utils.addCorsHeaders(new Response(stream, {
    status: 200,
    headers: {
      'Content-Type': 'text/event-stream',
      'Cache-Control': 'no-cache',
      'Connection': 'keep-alive'
    }
  }));
}

/**
 * å¤„ç†æ¨¡å‹åˆ—è¡¨è¯·æ±‚
 */
function handleModels() {
  const models = {
    object: "list",
    data: [
      { id: "gpt-3.5-turbo", object: "model", created: 1677610602, owned_by: "openai" },
      { id: "gpt-4", object: "model", created: 1687882411, owned_by: "openai" },
      { id: "gemini-2.5-flash", object: "model", created: 1687882411, owned_by: "google" },
      { id: "gemini-2.5-flash-lite", object: "model", created: 1687882411, owned_by: "google" },
      { id: "gemini-2.5-pro", object: "model", created: 1687882411, owned_by: "google" }
    ]
  };

  return utils.addCorsHeaders(new Response(utils.safeJsonStringify(models), {
    status: 200,
    headers: { 'Content-Type': 'application/json' }
  }));
}

/**
 * OpenAIæ¨¡å‹ååˆ°Geminiæ¨¡å‹åçš„æ˜ å°„
 */
function mapOpenAIModelToGemini(openaiModel) {
  const modelMap = {
    'gpt-3.5-turbo': 'gemini-2.5-flash',
    'gpt-4': 'gemini-2.5-pro',
    'gemini-2.5-flash': 'gemini-2.5-flash',
    'gemini-2.5-flash-lite': 'gemini-2.5-flash',
    'gemini-2.5-pro': 'gemini-2.5-pro'
  };

  return modelMap[openaiModel] || 'gemini-2.5-flash';
}

export default openai;
