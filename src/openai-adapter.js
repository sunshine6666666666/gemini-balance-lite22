/**
 * OpenAIå…¼å®¹é€‚é…å™¨ - çº¯ç²¹çš„æ ¼å¼è½¬æ¢å±‚
 * ä¸“æ³¨äºOpenAIåˆ°Geminiçš„æ ¼å¼è½¬æ¢ï¼Œç§»é™¤é‡å¤çš„éªŒè¯å’Œæ—¥å¿—é€»è¾‘
 */

import {
  generateRequestId,
  logRequestStart,
  logRequestEnd,
  logLLMRequest,
  logError,
  getEffectiveApiKeys,
  enhancedFetch,
  addCorsHeaders,
  safeJsonParse,
  safeJsonStringify
} from './utils.js';

/**
 * OpenAIå…¼å®¹APIä¸»å…¥å£
 */
const openai = {
  async fetch(request) {
    const reqId = generateRequestId();
    const startTime = Date.now();
    const url = new URL(request.url);
    
    try {
      logRequestStart(reqId, request.method, url.pathname, 0);

      // è·å–API Key
      const auth = request.headers.get("Authorization");
      const apiKey = auth?.split(" ")[1];
      if (!apiKey) {
        throw new Error('ç¼ºå°‘API Key');
      }

      // è·å–æœ‰æ•ˆçš„API Keyæ±  - ä¼ é€’å®Œæ•´çš„API Keyå­—ç¬¦ä¸²
      const apiKeys = getEffectiveApiKeys(apiKey, 'OpenAIæ¨¡å¼: ');
      console.log(`ğŸ”‘ [${reqId}] OpenAIæ¨¡å¼è·å¾—${apiKeys.length}ä¸ªæœ‰æ•ˆAPI Key`);

      // è·¯ç”±å¤„ç†
      switch (true) {
        case url.pathname.endsWith("/chat/completions"):
          console.log(`ğŸ’¬ [${reqId}] å¤„ç†èŠå¤©å®Œæˆè¯·æ±‚`);
          const chatBody = await request.json();
          return await handleChatCompletions(chatBody, apiKeys, reqId);

        case url.pathname.endsWith("/models"):
          console.log(`ğŸ“‹ [${reqId}] å¤„ç†æ¨¡å‹åˆ—è¡¨è¯·æ±‚`);
          return handleModels();

        case url.pathname.endsWith("/embeddings"):
          console.log(`ğŸ”¤ [${reqId}] å¤„ç†æ–‡æœ¬åµŒå…¥è¯·æ±‚`);
          const embedBody = await request.json();
          return await handleEmbeddings(embedBody, apiKeys[0], reqId);

        default:
          throw new Error(`ä¸æ”¯æŒçš„OpenAIç«¯ç‚¹: ${url.pathname}`);
      }

    } catch (error) {
      logError(reqId, error, 'OpenAIé€‚é…å™¨');
      return createErrorResponse(error, reqId);
    } finally {
      const totalTime = Date.now() - startTime;
      logRequestEnd(reqId, 200, totalTime);
    }
  }
};

/**
 * å¤„ç†èŠå¤©å®Œæˆè¯·æ±‚ - OpenAIåˆ°Geminiæ ¼å¼è½¬æ¢
 */
async function handleChatCompletions(openaiRequest, apiKeys, reqId) {
  console.log(`ğŸ”„ [${reqId}] å¼€å§‹OpenAIåˆ°Geminiæ ¼å¼è½¬æ¢`);

  // è½¬æ¢æ¶ˆæ¯æ ¼å¼
  const geminiContents = openaiRequest.messages.map(msg => ({
    role: msg.role === 'assistant' ? 'model' : msg.role,
    parts: [{ text: msg.content }]
  }));

  // æ„å»ºGeminiè¯·æ±‚
  const geminiRequest = {
    contents: geminiContents,
    generationConfig: {
      temperature: openaiRequest.temperature || 0.7,
      maxOutputTokens: openaiRequest.max_tokens || 1024,
      topP: openaiRequest.top_p || 1.0
    }
  };

  console.log(`ğŸ“ [${reqId}] è½¬æ¢å®Œæˆ: ${geminiContents.length}æ¡æ¶ˆæ¯`);

  // ç¡®å®šæ˜¯å¦ä¸ºæµå¼è¯·æ±‚
  const isStreaming = openaiRequest.stream === true;
  const endpoint = isStreaming ? 'streamGenerateContent' : 'generateContent';
  const model = mapOpenAIModelToGemini(openaiRequest.model);
  
  const targetUrl = `https://generativelanguage.googleapis.com/v1beta/models/${model}:${endpoint}`;
  logLLMRequest(reqId, targetUrl, model, isStreaming);

  // å‘é€è¯·æ±‚åˆ°Gemini
  const response = await enhancedFetch(
    targetUrl,
    {
      method: 'POST',
      headers: { 'Content-Type': 'application/json' },
      body: safeJsonStringify(geminiRequest)
    },
    apiKeys,
    reqId,
    'OpenAI->Gemini'
  );

  // å¤„ç†å“åº”
  if (isStreaming) {
    return handleStreamingResponse(response, reqId);
  } else {
    return handleNormalResponse(response, reqId);
  }
}

/**
 * å¤„ç†æ™®é€šå“åº” - Geminiåˆ°OpenAIæ ¼å¼è½¬æ¢
 */
async function handleNormalResponse(response, reqId) {
  const geminiResponse = await response.json();
  console.log(`ğŸ”„ [${reqId}] å¼€å§‹Geminiåˆ°OpenAIæ ¼å¼è½¬æ¢`);

  // è½¬æ¢ä¸ºOpenAIæ ¼å¼
  const openaiResponse = {
    id: `chatcmpl-${reqId}`,
    object: "chat.completion",
    created: Math.floor(Date.now() / 1000),
    model: "gpt-3.5-turbo",
    choices: [{
      index: 0,
      message: {
        role: "assistant",
        content: geminiResponse.candidates?.[0]?.content?.parts?.[0]?.text || "æŠ±æ­‰ï¼Œæ— æ³•ç”Ÿæˆå›å¤ã€‚"
      },
      finish_reason: "stop"
    }],
    usage: {
      prompt_tokens: 0,
      completion_tokens: 0,
      total_tokens: 0
    }
  };

  console.log(`âœ… [${reqId}] æ ¼å¼è½¬æ¢å®Œæˆ`);

  const newResponse = new Response(safeJsonStringify(openaiResponse), {
    status: 200,
    headers: { 'Content-Type': 'application/json' }
  });

  return addCorsHeaders(newResponse);
}

/**
 * å¤„ç†æµå¼å“åº” - Geminiåˆ°OpenAI SSEæ ¼å¼è½¬æ¢
 */
async function handleStreamingResponse(response, reqId) {
  console.log(`ğŸŒŠ [${reqId}] å¼€å§‹æµå¼å“åº”è½¬æ¢`);

  const reader = response.body.getReader();
  const encoder = new TextEncoder();
  const decoder = new TextDecoder();

  const stream = new ReadableStream({
    async start(controller) {
      try {
        while (true) {
          const { done, value } = await reader.read();
          if (done) break;

          const chunk = decoder.decode(value, { stream: true });
          const lines = chunk.split('\n').filter(line => line.trim());

          for (const line of lines) {
            if (line.startsWith('data: ')) {
              const data = line.slice(6);
              if (data === '[DONE]') continue;

              try {
                const geminiData = JSON.parse(data);
                const openaiChunk = convertGeminiToOpenAIChunk(geminiData, reqId);
                const sseData = `data: ${safeJsonStringify(openaiChunk)}\n\n`;
                controller.enqueue(encoder.encode(sseData));
              } catch (e) {
                console.log(`âš ï¸ [${reqId}] æµæ•°æ®è§£æå¤±è´¥: ${e.message}`);
              }
            }
          }
        }

        // å‘é€ç»“æŸæ ‡è®°
        controller.enqueue(encoder.encode('data: [DONE]\n\n'));
      } catch (error) {
        logError(reqId, error, 'æµå¼è½¬æ¢');
        controller.error(error);
      } finally {
        controller.close();
      }
    }
  });

  const streamResponse = new Response(stream, {
    status: 200,
    headers: {
      'Content-Type': 'text/event-stream',
      'Cache-Control': 'no-cache',
      'Connection': 'keep-alive'
    }
  });

  return addCorsHeaders(streamResponse);
}

/**
 * è½¬æ¢Geminiæµæ•°æ®å—ä¸ºOpenAIæ ¼å¼
 */
function convertGeminiToOpenAIChunk(geminiData, reqId) {
  return {
    id: `chatcmpl-${reqId}`,
    object: "chat.completion.chunk",
    created: Math.floor(Date.now() / 1000),
    model: "gpt-3.5-turbo",
    choices: [{
      index: 0,
      delta: {
        content: geminiData.candidates?.[0]?.content?.parts?.[0]?.text || ""
      },
      finish_reason: null
    }]
  };
}

/**
 * å¤„ç†æ¨¡å‹åˆ—è¡¨è¯·æ±‚
 */
function handleModels() {
  const models = {
    object: "list",
    data: [
      {
        id: "gpt-3.5-turbo",
        object: "model",
        created: 1677610602,
        owned_by: "openai"
      },
      {
        id: "gpt-4",
        object: "model", 
        created: 1687882411,
        owned_by: "openai"
      }
    ]
  };

  const response = new Response(safeJsonStringify(models), {
    status: 200,
    headers: { 'Content-Type': 'application/json' }
  });

  return addCorsHeaders(response);
}

/**
 * å¤„ç†åµŒå…¥è¯·æ±‚ï¼ˆç®€åŒ–ç‰ˆï¼‰
 */
async function handleEmbeddings(request, apiKey, reqId) {
  console.log(`ğŸ”¤ [${reqId}] åµŒå…¥åŠŸèƒ½æš‚ä¸æ”¯æŒ`);
  
  const response = {
    object: "list",
    data: [],
    model: "text-embedding-ada-002",
    usage: { prompt_tokens: 0, total_tokens: 0 }
  };

  const newResponse = new Response(safeJsonStringify(response), {
    status: 200,
    headers: { 'Content-Type': 'application/json' }
  });

  return addCorsHeaders(newResponse);
}

/**
 * OpenAIæ¨¡å‹åˆ°Geminiæ¨¡å‹æ˜ å°„
 */
function mapOpenAIModelToGemini(openaiModel) {
  const modelMap = {
    'gpt-3.5-turbo': 'gemini-2.5-flash',
    'gpt-4': 'gemini-2.5-pro',
    'gpt-4-turbo': 'gemini-2.5-pro'
  };
  
  return modelMap[openaiModel] || 'gemini-2.5-flash';
}

/**
 * åˆ›å»ºé”™è¯¯å“åº”
 */
function createErrorResponse(error, reqId) {
  const errorResponse = {
    error: {
      message: error.message,
      type: "openai_adapter_error",
      request_id: reqId
    }
  };

  const response = new Response(safeJsonStringify(errorResponse), {
    status: 500,
    headers: { 'Content-Type': 'application/json' }
  });

  return addCorsHeaders(response);
}

export default openai;
